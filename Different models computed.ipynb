{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "obvious-hormone",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exempt-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-spice",
   "metadata": {},
   "source": [
    "### Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sought-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('train_x_propre.csv', index_col=0)\n",
    "train_y = pd.read_csv('train_y_propre.csv', index_col=0)\n",
    "test_x = pd.read_csv('test_x_propre.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-equation",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "nervous-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adaboost classifier for each possible output\n",
    "\n",
    "#Updates only\n",
    "target = train_y['updates']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x) \n",
    "preds_proba_updates=model.predict_proba(test_x)\n",
    "\n",
    "#Personal only\n",
    "target = train_y['personal']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x)\n",
    "preds_proba_personal=model.predict_proba(test_x)\n",
    "\n",
    "#Promotions only\n",
    "target = train_y['promotions']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x)  \n",
    "preds_proba_promotions=model.predict_proba(test_x)\n",
    "\n",
    "#Forums only\n",
    "target = train_y['forums']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x)  \n",
    "preds_proba_forums=model.predict_proba(test_x)\n",
    "\n",
    "#Purchases only\n",
    "target = train_y['purchases']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x) \n",
    "preds_proba_purchases=model.predict_proba(test_x)\n",
    "\n",
    "#Travel only\n",
    "target = train_y['travel']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x) \n",
    "preds_proba_travel=model.predict_proba(test_x)\n",
    "\n",
    "#Spam only\n",
    "target = train_y['spam']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x)\n",
    "preds_proba_spam=model.predict_proba(test_x)\n",
    "\n",
    "#Social only\n",
    "target = train_y['social']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "model = abc.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_x)\n",
    "preds_proba_social=model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-standard",
   "metadata": {},
   "source": [
    "## Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "closing-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Updates Train accuracy 0.6388328711872951\n",
      "Logistic Regression Updates Test accuracy 0.6326402016383113\n",
      "Updates Log loss 0.655822193421087\n",
      "Logistic Regression Personal Train accuracy 0.810089488278296\n",
      "Logistic Regression Personal Test accuracy 0.8076874606175173\n",
      "Personal Log loss 0.5088838675983182\n",
      "Logistic Regression Promotions Train accuracy 0.799785732291404\n",
      "Logistic Regression Promotions Test accuracy 0.8020163831127914\n",
      "Promotions Log loss 0.5079247341589346\n",
      "Logistic Regression Forums Train accuracy 0.8586778421981346\n",
      "Logistic Regression Forums Test accuracy 0.8587271581600504\n",
      "Forums Log loss 0.37249246829508303\n",
      "Logistic Regression Purchases Train accuracy 0.9920594907990925\n",
      "Logistic Regression Purchases Test accuracy 0.9902961562696913\n",
      "Purchases Log loss 0.06172484903859077\n",
      "Logistic Regression Travel Train accuracy 0.997384673556844\n",
      "Logistic Regression Travel Test accuracy 0.9978575929426591\n",
      "Travel Log loss 0.018287868670458323\n",
      "Logistic Regression Spam Train accuracy 0.996218805142425\n",
      "Logistic Regression Spam Test accuracy 0.9959672337744171\n",
      "Spam Log loss 0.0328713898814758\n",
      "Logistic Regression Social Train accuracy 0.9056276783463575\n",
      "Logistic Regression Social Test accuracy 0.9069943289224953\n",
      "Social Log loss 0.23098164462568813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8991808443604284"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic Regression for each possible output\n",
    "\n",
    "# Updates only\n",
    "target = train_y['updates']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Updates Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Updates Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_updates=model.predict_proba(X_test)\n",
    "print('Updates Log loss %s' % log_loss(y_test,preds_proba_updates,labels=model.classes_))\n",
    "\n",
    "#Personal only\n",
    "target = train_y['personal']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Personal Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Personal Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_personal=model.predict_proba(X_test)\n",
    "print('Personal Log loss %s' % log_loss(y_test,preds_proba_personal,labels=model.classes_))\n",
    "\n",
    "#Promotions only\n",
    "target = train_y['promotions']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Promotions Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Promotions Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_promotions=model.predict_proba(X_test)\n",
    "print('Promotions Log loss %s' % log_loss(y_test,preds_proba_promotions,labels=model.classes_))\n",
    "\n",
    "#Forums only\n",
    "target = train_y['forums']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Forums Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Forums Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_forums=model.predict_proba(X_test)\n",
    "print('Forums Log loss %s' % log_loss(y_test,preds_proba_forums,labels=model.classes_))\n",
    "\n",
    "#Purchases only\n",
    "target = train_y['purchases']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Purchases Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Purchases Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_purchases=model.predict_proba(X_test)\n",
    "print('Purchases Log loss %s' % log_loss(y_test,preds_proba_purchases,labels=model.classes_))\n",
    "\n",
    "#Travel only\n",
    "target = train_y['travel']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Travel Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Travel Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_travel=model.predict_proba(X_test)\n",
    "print('Travel Log loss %s' % log_loss(y_test,preds_proba_travel,labels=model.classes_))\n",
    "\n",
    "#Spam only\n",
    "target = train_y['spam']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Spam Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Spam Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_spam=model.predict_proba(X_test)\n",
    "print('Spam Log loss %s' % log_loss(y_test,preds_proba_spam,labels=model.classes_))\n",
    "\n",
    "#Social only\n",
    "target = train_y['social']\n",
    "scaler = StandardScaler()\n",
    "train_x[['images','urls','chars_in_subject','chars_in_body']] = scaler.fit_transform(train_x[['images','urls','chars_in_subject','chars_in_body']])\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.2,random_state=42) \n",
    "model = lr.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "train_accuracy.append(model.score(X_train, y_train))\n",
    "test_accuracy.append(accuracy_score(preds, y_test))\n",
    "print('Logistic Regression Social Train accuracy %s' % model.score(X_train, y_train)) \n",
    "print('Logistic Regression Social Test accuracy %s' % accuracy_score(preds, y_test))  \n",
    "preds_proba_social=model.predict_proba(X_test)\n",
    "print('Social Log loss %s' % log_loss(y_test,preds_proba_social,labels=model.classes_))\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-report",
   "metadata": {},
   "source": [
    "# XG boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "imposed-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-resistance",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "removed-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:34:38] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[9.9993616e-01 6.3853491e-05]\n",
      " [9.8996794e-01 1.0032056e-02]\n",
      " [8.1791008e-01 1.8208994e-01]\n",
      " ...\n",
      " [9.9314868e-01 6.8513202e-03]\n",
      " [9.9986881e-01 1.3121298e-04]\n",
      " [9.3418771e-01 6.5812297e-02]]\n",
      "ok\n",
      "[22:34:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:34:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:34:58] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:35:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:35:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:35:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:35:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9093198992443325"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiclass XGBoost for each possible output\n",
    "\n",
    "#updates only\n",
    "target = train_y['updates']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "target = pd.to_numeric(target)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x)\n",
    "preds_proba_updates=model.predict_proba(test_x)\n",
    "print(preds_proba_updates)\n",
    "print('ok')\n",
    "\n",
    "\n",
    "#Personal only\n",
    "target = train_y['personal']\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x) \n",
    "preds_proba_personal=model.predict_proba(test_x)\n",
    "\n",
    "\n",
    "#Promotions only\n",
    "target = train_y['promotions']\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x) \n",
    "preds_proba_promotions=model.predict_proba(test_x)\n",
    "\n",
    "#Forums only\n",
    "target = train_y['forums']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x) \n",
    "preds_proba_forums=model.predict_proba(test_x)\n",
    "\n",
    "\n",
    "#Purchases only\n",
    "target = train_y['purchases']\n",
    "lr = LogisticRegression(random_state=0, max_iter=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x)\n",
    "preds_proba_purchases=model.predict_proba(test_x)\n",
    "\n",
    "\n",
    "#Travel only\n",
    "target = train_y['travel']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x) \n",
    "preds_proba_travel=model.predict_proba(test_x)\n",
    "\n",
    "\n",
    "#Spam only\n",
    "target = train_y['spam']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x)  \n",
    "preds_proba_spam=model.predict_proba(test_x)\n",
    "\n",
    "\n",
    "#Social only\n",
    "target = train_y['social']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, target, test_size = 0.01,random_state=42) \n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(test_x) \n",
    "preds_proba_social=model.predict_proba(test_x)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-creativity",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "brutal-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We implemented a model of neural network under the form of a multi layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes = (50,50,50), activation = 'logistic', random_state = 1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "italian-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9093198992443325"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-framework",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "competitive-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>model</th>\n",
       "      <th>classification_rate</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [family, model, classification_rate, runtime]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packages required for metrics\n",
    "import time, math\n",
    "from sklearn.metrics import accuracy_score\n",
    "cols_results=['family','model','classification_rate','runtime']\n",
    "results = pd.DataFrame(columns=cols_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-armenia",
   "metadata": {},
   "source": [
    "Split du dataset pour avoir 1/3 de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "western-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x,train_y,test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "laden-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>model</th>\n",
       "      <th>classification_rate</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-1</td>\n",
       "      <td>0.333486</td>\n",
       "      <td>9.221774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-2</td>\n",
       "      <td>0.258402</td>\n",
       "      <td>8.402228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-3</td>\n",
       "      <td>0.307134</td>\n",
       "      <td>9.473927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-4</td>\n",
       "      <td>0.268943</td>\n",
       "      <td>10.821194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-5</td>\n",
       "      <td>0.297815</td>\n",
       "      <td>10.698149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-6</td>\n",
       "      <td>0.263291</td>\n",
       "      <td>11.251910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-7</td>\n",
       "      <td>0.277880</td>\n",
       "      <td>11.082034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-8</td>\n",
       "      <td>0.255652</td>\n",
       "      <td>11.243865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-9</td>\n",
       "      <td>0.265964</td>\n",
       "      <td>11.589124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-1</td>\n",
       "      <td>0.333486</td>\n",
       "      <td>7.952696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-2</td>\n",
       "      <td>0.258402</td>\n",
       "      <td>8.464928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-3</td>\n",
       "      <td>0.307134</td>\n",
       "      <td>9.103653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-4</td>\n",
       "      <td>0.268943</td>\n",
       "      <td>10.836245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-5</td>\n",
       "      <td>0.297815</td>\n",
       "      <td>11.165042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-6</td>\n",
       "      <td>0.263291</td>\n",
       "      <td>11.134699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-7</td>\n",
       "      <td>0.277880</td>\n",
       "      <td>11.469363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-8</td>\n",
       "      <td>0.255652</td>\n",
       "      <td>11.449967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-9</td>\n",
       "      <td>0.265964</td>\n",
       "      <td>11.781833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-10</td>\n",
       "      <td>0.249389</td>\n",
       "      <td>11.850770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-11</td>\n",
       "      <td>0.258708</td>\n",
       "      <td>11.867255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-12</td>\n",
       "      <td>0.243431</td>\n",
       "      <td>11.998332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-13</td>\n",
       "      <td>0.252750</td>\n",
       "      <td>12.887864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNN-14</td>\n",
       "      <td>0.239536</td>\n",
       "      <td>11.958549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   family   model  classification_rate    runtime\n",
       "0     KNN   KNN-1             0.333486   9.221774\n",
       "1     KNN   KNN-2             0.258402   8.402228\n",
       "2     KNN   KNN-3             0.307134   9.473927\n",
       "3     KNN   KNN-4             0.268943  10.821194\n",
       "4     KNN   KNN-5             0.297815  10.698149\n",
       "5     KNN   KNN-6             0.263291  11.251910\n",
       "6     KNN   KNN-7             0.277880  11.082034\n",
       "7     KNN   KNN-8             0.255652  11.243865\n",
       "8     KNN   KNN-9             0.265964  11.589124\n",
       "9     KNN   KNN-1             0.333486   7.952696\n",
       "10    KNN   KNN-2             0.258402   8.464928\n",
       "11    KNN   KNN-3             0.307134   9.103653\n",
       "12    KNN   KNN-4             0.268943  10.836245\n",
       "13    KNN   KNN-5             0.297815  11.165042\n",
       "14    KNN   KNN-6             0.263291  11.134699\n",
       "15    KNN   KNN-7             0.277880  11.469363\n",
       "16    KNN   KNN-8             0.255652  11.449967\n",
       "17    KNN   KNN-9             0.265964  11.781833\n",
       "18    KNN  KNN-10             0.249389  11.850770\n",
       "19    KNN  KNN-11             0.258708  11.867255\n",
       "20    KNN  KNN-12             0.243431  11.998332\n",
       "21    KNN  KNN-13             0.252750  12.887864\n",
       "22    KNN  KNN-14             0.239536  11.958549"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kVals = range(1,15)\n",
    "knn_names = ['KNN-'+str(k) for k in kVals]\n",
    "for k in kVals:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    time_start = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    time_run = time.time()-time_start\n",
    "    \n",
    "    results = results.append(pd.DataFrame([['KNN',knn_names[k-1],accuracy_score(y_test,y_pred),time_run]],columns=cols_results),ignore_index=True)\n",
    "results[results.family=='KNN']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
